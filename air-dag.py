# -*- coding: utf-8 -*-
"""drive_bq

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u5wOIoGnBvsyi1ECIbeAS_qRApLYNGyj
"""

import pandas as pd
import numpy as np
from google.cloud import storage
import io
import os
import logging
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.hooks.base import BaseHook
from datetime import datetime, timedelta

# Configure basic logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# DAG default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

# Configuration variables
BUCKET_NAME = "us-central1-healthcare-env-910bc212-bucket"
RAW_FOLDER_OPTIONS = [
    "data/raw_data/data_csv",            # New path where files are located
    "data/raw_data/mimic-iv-ed-2.2/ed",  # Original path
    "raw_data/mimic-iv-ed-2.2/ed",       # Alternative path without 'data' prefix
    "mimic-iv-ed-2.2/ed",                # Another possible path
    ""                                    # Root of bucket
]
CLEAN_FOLDER = "data/cleaned_data"
ALTERNATE_CLEAN_FOLDER = "cleaned_data"  # Alternative path without 'data' prefix
SAMPLE_SIZE = 500  # Changed from 1000 to 500

# ========== CLEANING FUNCTIONS ==========

def initialize_storage_client():
    """Initialize Google Cloud Storage client and bucket"""
    try:
        logging.info("Initializing GCS client...")
        storage_client = storage.Client()
        bucket = storage_client.bucket(BUCKET_NAME)
        logging.info(f"Successfully connected to bucket: {BUCKET_NAME}")
        return storage_client, bucket
    except Exception as e:
        logging.error(f"Failed to initialize storage client: {e}")
        raise

def list_blobs_with_prefix(bucket, prefix=""):
    """List all blobs in the bucket with the given prefix"""
    try:
        logging.info(f"Listing blobs with prefix: '{prefix}'")
        blobs = list(bucket.list_blobs(prefix=prefix))
        logging.info(f"Found {len(blobs)} blobs with prefix '{prefix}'")

        # Log the first 10 blob names
        for i, blob in enumerate(blobs[:10]):
            logging.info(f"Blob {i+1}: {blob.name}")

        return blobs
    except Exception as e:
        logging.error(f"Failed to list blobs with prefix '{prefix}': {e}")
        raise

def search_for_csv_files(bucket):
    """Search for CSV files in various locations to find the raw data"""
    try:
        logging.info("Searching for CSV files in the bucket...")

        # Search in each potential raw folder path
        for folder in RAW_FOLDER_OPTIONS:
            logging.info(f"Checking folder: '{folder}'")
            blobs = list_blobs_with_prefix(bucket, folder)

            # Log all blobs found in this folder (up to 20)
            logging.info(f"Found {len(blobs)} files in folder '{folder}'")
            for i, blob in enumerate(blobs[:20]):
                logging.info(f"- {blob.name}")

            # Check if any of these blobs are CSV files we need
            csv_files = [
                "diagnosis.csv", "edstays.csv", "medrecon.csv",
                "pyxis.csv", "triage.csv", "vitalsign.csv"
            ]

            found_csvs = []
            for csv_file in csv_files:
                for blob in blobs:
                    if blob.name.endswith(csv_file):
                        found_csvs.append((folder, blob.name))
                        logging.info(f"Found CSV file: {blob.name}")
                        break

            if found_csvs:
                logging.info(f"Found {len(found_csvs)} CSV files in folder '{folder}'")
                for folder_path, file_path in found_csvs:
                    logging.info(f"  - {file_path}")
                return folder, found_csvs

        # If we get here, we couldn't find the CSV files
        logging.error("Could not find required CSV files in any of the expected locations")
        # Log all RAW_FOLDER_OPTIONS to help with debugging
        logging.error(f"Searched in these folders: {RAW_FOLDER_OPTIONS}")
        return None, []
    except Exception as e:
        logging.error(f"Error searching for CSV files: {e}")
        raise

def download_blob_to_df(bucket, blob_name):
    """Download a blob from GCS bucket and convert to pandas DataFrame"""
    try:
        logging.info(f"Downloading {blob_name}...")
        blob = bucket.blob(blob_name)
        if not blob.exists():
            logging.error(f"Blob not found: {blob_name}")
            raise FileNotFoundError(f"Blob not found: {blob_name}")

        # Log the blob size before downloading
        blob.reload()  # Refresh metadata
        logging.info(f"Blob size: {blob.size} bytes")

        content = blob.download_as_string()

        # Log content size after download
        logging.info(f"Downloaded content size: {len(content)} bytes")

        # Parse the CSV data
        df = pd.read_csv(io.BytesIO(content))

        # Log detailed information about the DataFrame
        logging.info(f"Downloaded {blob_name} - Shape: {df.shape}")
        logging.info(f"DataFrame columns: {list(df.columns)}")
        logging.info(f"Data types: {df.dtypes}")

        # Count unique values for important ID columns if they exist
        id_columns = ['subject_id', 'stay_id', 'hadm_id']
        for col in id_columns:
            if col in df.columns:
                unique_count = df[col].nunique()
                logging.info(f"Number of unique values in {col}: {unique_count}")

        return df
    except Exception as e:
        logging.error(f"Failed to download {blob_name}: {e}")
        logging.error(f"Exception details: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())
        raise

def upload_df_to_blob(bucket, df, fname):
    """Upload a pandas DataFrame to GCS bucket"""
    try:
        gcs_path = f"{CLEAN_FOLDER}/{fname}"  # Full GCS path
        logging.info(f"Uploading to: {gcs_path} ...")
        blob = bucket.blob(gcs_path)
        csv_content = df.to_csv(index=False)

        # Log a preview of the data being uploaded
        if len(csv_content) > 200:
            logging.info(f"CSV content preview (first 200 chars): {csv_content[:200]}")
        else:
            logging.info(f"CSV content: {csv_content}")

        blob.upload_from_string(csv_content, 'text/csv')
        logging.info(f"Successfully uploaded {gcs_path}")

        # Verify upload by trying to retrieve metadata
        try:
            blob.reload()  # Refresh metadata
            logging.info(f"Verified blob exists after upload: {gcs_path}, size: {blob.size} bytes")
        except Exception as e:
            logging.error(f"Failed to verify upload of {gcs_path}: {e}")
    except Exception as e:
        logging.error(f"Failed to upload {gcs_path}: {e}")
        raise

def create_clean_folder_if_not_exists(bucket):
    """Create the cleaned_data folder in GCS if it doesn't exist"""
    try:
        logging.info(f"Creating folder if not exists: {CLEAN_FOLDER}")
        blob = bucket.blob(f"{CLEAN_FOLDER}/.placeholder")
        if not blob.exists():
            logging.info(f"Creating folder: {CLEAN_FOLDER}")
            blob.upload_from_string('', content_type='text/plain')
            logging.info(f"Successfully created folder: {CLEAN_FOLDER}")
        else:
            logging.info(f"Folder already exists: {CLEAN_FOLDER}")
    except Exception as e:
        logging.error(f"Failed to create folder {CLEAN_FOLDER}: {e}")
        raise

def generate_test_data():
    """Generate some test data to use if we can't find the real data"""
    logging.info("Generating test data...")

    # edstays.csv
    edstays = pd.DataFrame({
        'subject_id': range(1, 11),
        'hadm_id': [None, 2, 3, None, 5, 6, 7, 8, 9, 10],
        'stay_id': range(101, 111),
        'gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'],
        'race': ['WHITE', 'BLACK', 'HISPANIC', 'ASIAN', 'OTHER', 'WHITE', 'BLACK', 'HISPANIC', 'ASIAN', 'OTHER'],
        'arrival_transport': ['AMBULANCE', 'WALK-IN', 'AMBULANCE', 'WALK-IN', 'AMBULANCE', 'WALK-IN', 'AMBULANCE', 'WALK-IN', 'AMBULANCE', 'WALK-IN'],
        'disposition': ['ADMIT', 'DISCHARGE', 'ADMIT', 'DISCHARGE', 'ADMIT', 'DISCHARGE', 'ADMIT', 'DISCHARGE', 'ADMIT', 'DISCHARGE'],
        'intime': [str(d) for d in pd.date_range(start='2023-01-01', periods=10, freq='D')],
        'outtime': [str(d) for d in pd.date_range(start='2023-01-02', periods=10, freq='D')]
    })

    # triage.csv
    triage = pd.DataFrame({
        'subject_id': range(1, 11),
        'stay_id': range(101, 111),
        'temperature': [98.6, 99.1, 97.9, 98.2, 101.3, 98.6, 99.5, 97.8, 98.7, 99.2],
        'heartrate': [75, 82, 68, 90, 110, 72, 85, 79, 88, 65],
        'resprate': [16, 18, 15, 20, 24, 17, 19, 16, 18, 15],
        'o2sat': [98, 97, 99, 96, 95, 98, 97, 99, 98, 97],
        'sbp': [120, 140, 110, 130, 150, 125, 135, 115, 145, 120],
        'dbp': [80, 90, 70, 85, 95, 75, 85, 75, 90, 80],
        'pain': [0, 2, 5, 7, 10, 3, 6, 4, 2, 8],
        'acuity': [3, 2, 4, 3, 5, 2, 3, 4, 2, 3],
        'chiefcomplaint': ['CHEST PAIN', 'FEVER', 'ABDOMINAL PAIN', 'HEADACHE', 'SHORTNESS OF BREATH',
                          'BACK PAIN', 'VOMITING', 'DIZZINESS', 'FALL', 'LACERATION']
    })

    # Minimal versions of remaining files
    vitalsign = pd.DataFrame({
        'subject_id': range(1, 11),
        'stay_id': range(101, 111),
        'charttime': [str(d) for d in pd.date_range(start='2023-01-01', periods=10, freq='H')],
        'temperature': [98.6, 99.1, 97.9, 98.2, 101.3, 98.6, 99.5, 97.8, 98.7, 99.2],
        'heartrate': [75, 82, 68, 90, 110, 72, 85, 79, 88, 65]
    })

    diagnosis = pd.DataFrame({
        'subject_id': range(1, 11),
        'stay_id': range(101, 111),
        'icd_code': ['I10', 'J18.9', 'R07.9', 'K92.2', 'R55', 'R51', 'K59.0', 'R42', 'W19', 'S01.9'],
        'icd_title': ['HYPERTENSION', 'PNEUMONIA', 'CHEST PAIN', 'GI BLEED', 'SYNCOPE',
                      'HEADACHE', 'CONSTIPATION', 'DIZZINESS', 'FALL', 'LACERATION']
    })

    medrecon = pd.DataFrame({
        'subject_id': range(1, 11),
        'stay_id': range(101, 111),
        'charttime': [str(d) for d in pd.date_range(start='2023-01-01', periods=10, freq='H')],
        'name': ['acetaminophen', 'lisinopril', 'metformin', 'aspirin', 'atorvastatin',
                'metoprolol', 'amlodipine', 'omeprazole', 'glipizide', 'furosemide'],
        'gsn': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
        'ndc': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
        'etcdescription': ['pain reliever', 'blood pressure', 'diabetes', 'blood thinner', 'cholesterol',
                          'heart', 'blood pressure', 'acid reflux', 'diabetes', 'diuretic']
    })

    pyxis = pd.DataFrame({
        'subject_id': range(1, 11),
        'stay_id': range(101, 111),
        'charttime': [str(d) for d in pd.date_range(start='2023-01-01', periods=10, freq='H')],
        'name': ['ACETAMINOPHEN', 'MORPHINE', 'ONDANSETRON', 'KETOROLAC', 'CEFTRIAXONE',
                'VANCOMYCIN', 'ALBUTEROL', 'INSULIN', 'LORAZEPAM', 'DIPHENHYDRAMINE'],
        'gsn': [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
    })

    return {
        'edstays.csv': edstays,
        'triage.csv': triage,
        'vitalsign.csv': vitalsign,
        'diagnosis.csv': diagnosis,
        'medrecon.csv': medrecon,
        'pyxis.csv': pyxis
    }

def run_cleaning_job(**context):
    """Main function to orchestrate the cleaning job"""
    start_time = datetime.now()
    logging.info(f"=== Starting cleaning job at {start_time} ===")
    logging.info(f"Using bucket: {BUCKET_NAME}")
    logging.info(f"Clean folder: {CLEAN_FOLDER}")

    try:
        # Initialize client and get bucket
        storage_client, bucket = initialize_storage_client()

        # First, list all the blobs to get a sense of what's in the bucket
        logging.info("Listing top-level folders in the bucket...")
        list_blobs_with_prefix(bucket, "")

        # Search for the raw data files
        logging.info("Searching for raw data files...")
        raw_folder_found, found_csvs = search_for_csv_files(bucket)

        # Create cleaned_data folder if it doesn't exist
        create_clean_folder_if_not_exists(bucket)

        # Process the found files or use test data if not found
        if not found_csvs:
            logging.warning("Could not find raw data files. Using test data instead.")
            test_data = generate_test_data()

            # Upload test data directly
            for filename, df in test_data.items():
                try:
                    logging.info(f"Processing test data for {filename}")
                    upload_df_to_blob(bucket, df, filename)
                except Exception as e:
                    logging.error(f"Error processing test data for {filename}: {e}")
        else:
            # Process the actual files that were found
            for raw_folder_path, file_path in found_csvs:
                try:
                    filename = os.path.basename(file_path)
                    logging.info(f"Processing {filename} from {file_path}")

                    # Download the file
                    df = download_blob_to_df(bucket, file_path)

                    # Log information about the raw data
                    logging.info(f"Raw data shape for {filename}: {df.shape}")
                    logging.info(f"First few rows of {filename}:")
                    logging.info(f"{df.head(3)}")

                    # Log sample column values if text columns exist
                    for col in df.columns:
                        if df[col].dtype == 'object' and len(df[col]) > 0:
                            sample_value = str(df[col].iloc[0])
                            sample_value = sample_value[:100] + '...' if len(sample_value) > 100 else sample_value
                            logging.info(f"Sample value for column '{col}': {sample_value}")

                    # Apply basic cleaning (just as an example)
                    df_clean = df.copy()

                    # Special handling for text columns
                    for col in df_clean.columns:
                        if df_clean[col].dtype == 'object':
                            df_clean[col] = df_clean[col].astype(str).str.strip().str.upper()

                    # Upload the cleaned file
                    upload_df_to_blob(bucket, df_clean, filename)

                    # Verify uploaded data size
                    logging.info(f"Cleaned data shape for {filename}: {df_clean.shape}")

                except Exception as e:
                    logging.error(f"Error processing {file_path}: {e}")
                    logging.error(f"Exception details: {str(e)}")
                    import traceback
                    logging.error(traceback.format_exc())

        # List files in clean folder to verify they were created
        logging.info(f"Listing files in clean folder: {CLEAN_FOLDER}")
        clean_blobs = list_blobs_with_prefix(bucket, CLEAN_FOLDER)

        # Log file sizes in clean folder
        logging.info("Files in clean folder:")
        for blob in clean_blobs:
            if blob.name.endswith('.csv'):
                logging.info(f"- {blob.name}: {blob.size} bytes")

        end_time = datetime.now()
        duration = end_time - start_time
        logging.info(f"=== Cleaning job completed at {end_time} ===")
        logging.info(f"=== Total duration: {duration} ===")

        return {
            "status": "success",
            "duration_seconds": duration.total_seconds()
        }

    except Exception as e:
        logging.error(f"Cleaning job failed: {e}")
        raise

# ========== BIGQUERY FUNCTIONS ==========

def upload_data_to_bigquery(**context):
    """Upload cleaned CSV files directly to BigQuery"""
    try:
        from google.cloud import bigquery

        # Initialize storage client
        storage_client = storage.Client()
        bucket = storage_client.bucket(BUCKET_NAME)

        # Initialize BigQuery client
        bq_client = bigquery.Client()

        # Create dataset if it doesn't exist
        dataset_id = f"{bq_client.project}.ed_data"
        try:
            bq_client.get_dataset(dataset_id)
            logging.info(f"Dataset {dataset_id} already exists")
        except Exception:
            dataset = bigquery.Dataset(dataset_id)
            dataset.location = "US"  # Set your desired location
            dataset = bq_client.create_dataset(dataset, exists_ok=True)
            logging.info(f"Created dataset {dataset_id}")

        # Get list of CSV files in the clean folder
        csv_files = []
        try:
            # Try primary folder first
            blobs = list(bucket.list_blobs(prefix=CLEAN_FOLDER))
            csv_files = [blob.name for blob in blobs if blob.name.endswith('.csv')]
            if not csv_files:
                # Try alternate folder if primary folder has no CSVs
                blobs = list(bucket.list_blobs(prefix=ALTERNATE_CLEAN_FOLDER))
                csv_files = [blob.name for blob in blobs if blob.name.endswith('.csv')]
        except Exception as e:
            logging.error(f"Error listing CSV files: {e}")
            raise

        if not csv_files:
            logging.error("No CSV files found in either folder")
            raise ValueError("No CSV files found to upload to BigQuery")

        logging.info(f"Found {len(csv_files)} CSV files to upload to BigQuery")

        # Upload each CSV file to a corresponding table in BigQuery
        for csv_file in csv_files:
            try:
                # Get table name from file name (remove path and extension)
                table_name = os.path.basename(csv_file).replace('.csv', '')

                # Create fully qualified table ID
                table_id = f"{dataset_id}.{table_name}"

                # Get blob size for logging
                blob = bucket.blob(csv_file)
                blob.reload()
                logging.info(f"Uploading {csv_file} to BigQuery (size: {blob.size} bytes)")

                # Configure load job
                job_config = bigquery.LoadJobConfig(
                    source_format=bigquery.SourceFormat.CSV,
                    skip_leading_rows=1,  # Skip header row
                    autodetect=True,  # Automatically detect schema
                    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE  # Overwrite if exists
                )

                # GCS URI for the file
                uri = f"gs://{BUCKET_NAME}/{csv_file}"

                # Load the CSV into BigQuery
                load_job = bq_client.load_table_from_uri(
                    uri, table_id, job_config=job_config
                )

                # Wait for job to complete
                result = load_job.result()

                # Log any errors that occurred
                if load_job.errors:
                    logging.error(f"Errors loading {csv_file} to BigQuery: {load_job.errors}")

                # Get the table and report the number of rows
                table = bq_client.get_table(table_id)
                logging.info(f"Loaded {table.num_rows} rows into {table_id}")

                # Check if the row count seems reasonable
                if table.num_rows < 10:
                    logging.warning(f"Table {table_id} has only {table.num_rows} rows. This might indicate a problem.")

            except Exception as e:
                logging.error(f"Error uploading {csv_file} to BigQuery: {e}")
                logging.error(f"Exception details: {str(e)}")
                import traceback
                logging.error(traceback.format_exc())
                # Continue with next file instead of failing completely
                continue

        # Track which tables were created successfully
        created_tables = []

        # Check which tables were created successfully
        for csv_file in csv_files:
            table_name = os.path.basename(csv_file).replace('.csv', '')
            table_id = f"{dataset_id}.{table_name}"
            try:
                table = bq_client.get_table(table_id)
                created_tables.append(f"{table_name} ({table.num_rows} rows)")
            except Exception:
                pass

        logging.info(f"Successfully created the following tables: {', '.join(created_tables)}")

        return {
            "status": "success",
            "tables_created": created_tables,
            "destination": "bigquery",
            "dataset": "ed_data"
        }

    except Exception as e:
        logging.error(f"Failed to upload data to BigQuery: {e}")
        raise

def create_patient_sample(**context):
    """Create a sample in BigQuery with up to 1000 unique subject_ids and export to CSV for fine-tuning"""
    try:
        from google.cloud import bigquery

        # Initialize clients
        bq_client = bigquery.Client()
        storage_client = storage.Client()
        bucket = storage_client.bucket(BUCKET_NAME)
        dataset_id = f"{bq_client.project}.ed_data"

        # Check if edstays table exists
        try:
            bq_client.get_table(f"{dataset_id}.edstays")
        except Exception as e:
            logging.error(f"Table edstays not found: {e}")
            raise ValueError("Table edstays not found. Sample cannot be created.")

        # First, check how many unique subject_ids are available
        count_query = f"""
        SELECT COUNT(DISTINCT subject_id) as total_patients
        FROM `{dataset_id}.edstays`
        """

        count_job = bq_client.query(count_query)
        count_result = list(count_job.result())[0]
        total_patients = count_result.total_patients

        logging.info(f"Found {total_patients} unique patients in the edstays table")

        # Determine the actual sample size (minimum of SAMPLE_SIZE and total_patients)
        actual_sample_size = min(SAMPLE_SIZE, total_patients)
        logging.info(f"Will create a sample with {actual_sample_size} patients")

        # If total_patients is small and we're working with real data, log a warning
        if total_patients < SAMPLE_SIZE:
            logging.warning(f"Only {total_patients} patients found, which is less than the requested {SAMPLE_SIZE}")
            logging.warning("Make sure your raw data files are correctly located in the bucket")

        # Create a sample table with unique subject_ids
        sample_query = f"""
        CREATE OR REPLACE TABLE `{dataset_id}.sample_patients` AS
        SELECT DISTINCT subject_id
        FROM `{dataset_id}.edstays`
        ORDER BY subject_id
        LIMIT {actual_sample_size};
        """

        query_job = bq_client.query(sample_query)
        query_job.result()

        # Create a comprehensive view joining all 6 tables
        view_query = f"""
        CREATE OR REPLACE VIEW `{dataset_id}.sample_data_view` AS
        SELECT
            e.*,
            t.temperature as triage_temperature,
            t.heartrate as triage_heartrate,
            t.resprate as triage_resprate,
            t.o2sat as triage_o2sat,
            t.pain as triage_pain,
            t.acuity as triage_acuity,
            t.chiefcomplaint as triage_chiefcomplaint,
            d.icd_code,
            d.icd_title,
            v.temperature as vs_temperature,
            v.heartrate as vs_heartrate,
            v.charttime as vs_charttime,
            m.name as med_name,
            m.gsn as med_gsn,
            m.ndc as med_ndc,
            m.etcdescription as med_description,
            m.charttime as med_charttime,
            p.name as pyxis_name,
            p.gsn as pyxis_gsn,
            p.charttime as pyxis_charttime
        FROM
            `{dataset_id}.edstays` e
        LEFT JOIN
            `{dataset_id}.triage` t ON e.stay_id = t.stay_id
        LEFT JOIN
            `{dataset_id}.diagnosis` d ON e.stay_id = d.stay_id
        LEFT JOIN
            `{dataset_id}.vitalsign` v ON e.stay_id = v.stay_id
        LEFT JOIN
            `{dataset_id}.medrecon` m ON e.stay_id = m.stay_id
        LEFT JOIN
            `{dataset_id}.pyxis` p ON e.stay_id = p.stay_id
        WHERE
            e.subject_id IN (SELECT subject_id FROM `{dataset_id}.sample_patients`)
        """

        view_job = bq_client.query(view_query)
        view_job.result()

        # Create a materialized table from the view
        materialized_query = f"""
        CREATE OR REPLACE TABLE `{dataset_id}.sample_data` AS
        SELECT * FROM `{dataset_id}.sample_data_view`
        """

        materialized_job = bq_client.query(materialized_query)
        materialized_job.result()

        # Get count of records in materialized table
        table_count_query = f"""
        SELECT COUNT(*) as row_count
        FROM `{dataset_id}.sample_data`
        """

        table_count_job = bq_client.query(table_count_query)
        table_count_result = list(table_count_job.result())[0]
        table_row_count = table_count_result.row_count

        logging.info(f"Created sample_data table with {table_row_count} rows for {actual_sample_size} patients")

        # Get table reference for extraction
        table_ref = bq_client.dataset(dataset_id.split('.')[-1]).table("sample_data")

        # Set up extraction job to GCS
        destination_uri = f"gs://{BUCKET_NAME}/data/sample_data_*.csv"
        job_config = bigquery.ExtractJobConfig()
        job_config.destination_format = bigquery.DestinationFormat.CSV
        job_config.print_header = True

        # Start extraction job
        logging.info(f"Exporting sample data to {destination_uri}")
        extract_job = bq_client.extract_table(
            table_ref,
            destination_uri,
            job_config=job_config
        )

        # Wait for the job to complete
        extract_job.result()

        # Verify the export by listing blobs
        blobs = list(bucket.list_blobs(prefix="data/sample_data_"))
        exported_files = [blob.name for blob in blobs if blob.name.startswith("data/sample_data_")]

        if exported_files:
            logging.info(f"Successfully exported {len(exported_files)} file(s):")
            for file in exported_files:
                logging.info(f"- gs://{BUCKET_NAME}/{file}")

                # Get file size information
                blob = bucket.blob(file)
                logging.info(f"  File size: {blob.size} bytes")
        else:
            logging.warning("No export files found. Export may have failed.")

        return {
            "status": "success",
            "target_sample_size": SAMPLE_SIZE,
            "actual_sample_size": actual_sample_size,
            "total_unique_patients": total_patients,
            "rows_in_sample": table_row_count,
            "destination": "gcs",
            "dataset": "ed_data",
            "sample_table": "sample_patients",
            "sample_data_table": "sample_data",
            "csv_location": destination_uri
        }

    except Exception as e:
        logging.error(f"Failed to create sample in BigQuery and export to CSV: {e}")
        raise

# Define the DAG
with DAG(
    'healthcare_data_pipeline',
    default_args=default_args,
    description='Pipeline to clean, upload to BigQuery, and create sample healthcare data',
    schedule_interval=None,  # Set to None for manual triggers or '@daily', etc. for scheduled runs
    start_date=datetime(2023, 1, 1),
    catchup=False,
    tags=['healthcare', 'bigquery', 'etl'],
) as dag:

    # Task 1: Clean raw data
    clean_data_task = PythonOperator(
        task_id='clean_raw_data',
        python_callable=run_cleaning_job,
        provide_context=True,
    )

    # Task 2: Upload cleaned data to BigQuery
    upload_data_task = PythonOperator(
        task_id='upload_data_to_bigquery',
        python_callable=upload_data_to_bigquery,
        provide_context=True,
    )

    # Task 3: Create and store sample dataset
    create_sample_task = PythonOperator(
        task_id='create_patient_sample',
        python_callable=create_patient_sample,
        provide_context=True,
    )

    # Set task dependencies
    clean_data_task >> upload_data_task >> create_sample_task