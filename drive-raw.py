# -*- coding: utf-8 -*-
"""0_upload.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lvtqFnDMpNDTY340zYyvTEP-l1G3fUlG
"""

import os
import json
import zipfile
from google.colab import drive

# ========== CONFIGURATION ==========
# Set this to match your service account path inside Google Drive
service_account_key_path = "/content/drive/MyDrive/Project-A/sunlit_hook.json"

# Set your target Composer bucket and destination path
composer_bucket = "us-central1-healthcare-env-910bc212-bucket"
gcs_target_path = f"gs://{composer_bucket}/data/raw_data/"

# Mounted Drive path to ZIP file
zip_path = "/content/drive/MyDrive/Project-A/mimic-iv-ed.zip"
extract_path = "/content/mimic_raw_data"

# ========== STEP 1: Authenticate ==========
# Set up service account credentials
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = service_account_key_path

# Mount Drive
drive.mount('/content/drive')

# Load project ID
with open(service_account_key_path) as f:
    key_data = json.load(f)
project_id = key_data.get("project_id")
print("Your GCP Project ID is:", project_id)

# Activate gcloud credentials
!gcloud auth activate-service-account --key-file="$service_account_key_path"
!gcloud config set project $project_id

# ========== STEP 2: Unzip the Dataset ==========
os.makedirs(extract_path, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Dataset unzipped to:", extract_path)

# ========== STEP 3: Upload to Composer GCS Bucket ==========
print(f"Uploading to GCS path: {gcs_target_path}")
!gsutil -m cp -r /content/mimic_raw_data/* $gcs_target_path
print("Upload complete!")